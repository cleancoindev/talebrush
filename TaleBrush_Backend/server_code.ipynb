{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv1-cl0RWU-V"
      },
      "outputs": [],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask_api\n",
        "!pip install pyngrok\n",
        "!pip install flask_cors\n",
        "!pip install transformers==4.8.2\n",
        "# !ngrok authtoken [put your token for ngrok]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbD8x7n-Wv1M"
      },
      "outputs": [],
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import request, url_for\n",
        "from flask_api import FlaskAPI, status, exceptions\n",
        "from flask_cors import CORS, cross_origin\n",
        "import torch\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9dyKS886b_"
      },
      "source": [
        "# Generation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7xygA6R89XE"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/naver-ai/TaleBrush.git\n",
        "%cd TaleBrush\n",
        "%cd TaleBrush_Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI5Bq6UW9EP8"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from modeling_gptneo import GPTNeoForCausalLM\n",
        "from modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    GPTNeoConfig,\n",
        "    GPT2Config,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEwp3O-79U_K"
      },
      "outputs": [],
      "source": [
        "# download GeDi model \n",
        "!gdown --id 1rQn9uDlKPHi5dbzhCLGpYld5c2NM1L9i\n",
        "!unzip fortune_gedi.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqclUJPj9dXD"
      },
      "outputs": [],
      "source": [
        "# ptuning continuation model\n",
        "!gdown --id 1uHDpHpi95MNxQFE4sDQYcFKO3FYTLN-9\n",
        "!unzip continuation_Base_gptneo_ROC_prompt_candi3_lr7e-05.zip\n",
        "!mv ./result_embedding ./result_embedding_cont"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c664STmg9ket"
      },
      "outputs": [],
      "source": [
        "# ptuning infilling front model\n",
        "!gdown --id 1h5XMqEhBXOBv11gko25z1pn1O-MTl9zc\n",
        "!unzip infilling_front_Base_gptneo_ROC_prompt_lr7e-05.zip\n",
        "!mv ./result_embedding ./result_embedding_infill_front"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFLndQgK9oJR"
      },
      "outputs": [],
      "source": [
        "# ptuning infilling back model\n",
        "!gdown --id 1r_8wNM8v45HJ6xnTMwQf7F4FKlYk86y0\n",
        "!unzip infilling_back_Base_gptneo_ROC_prompt_lr7e-05.zip\n",
        "!mv ./result_embedding ./result_embedding_infill_back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OKo7U8MVHKC"
      },
      "outputs": [],
      "source": [
        "# ptuning recognition\n",
        "!gdown --id 1QqTS5PAV6i5iZAN8rlg9UQvjIgOxTYKd\n",
        "!unzip recognition_fortune.zip\n",
        "# !gdown --id 1ADZpN6JchwT9uZhlyTaYDvG5Q6zNLYXQ\n",
        "# !unzip recognition_fortune_test2.zip\n",
        "!mv ./result_embedding ./result_embedding_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PUQZW7_9wm9"
      },
      "outputs": [],
      "source": [
        "code_desired = \"true\"\n",
        "code_undesired = \"false\"\n",
        "model_type = 'gpt2'\n",
        "gen_type = \"gedi\"\n",
        "gen_model_name_or_path = \"EleutherAI/gpt-neo-2.7B\" #'./finetuned' #'./continuation_Base_gptneo_ROC' #\"gpt2-medium\"  'gpt-neo-2.7B'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
        "MODEL_CLASSES = {\"gptneo\": (GPTNeoConfig, GPTNeoForCausalLM, GPT2Tokenizer), \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
        "config_class_n, model_class_n, tokenizer_class_n = MODEL_CLASSES[\"gptneo\"]\n",
        "config_class_2, model_class_2, tokenizer_class_2 = MODEL_CLASSES[\"gpt2\"]\n",
        "tokenizer = tokenizer_class_n.from_pretrained('EleutherAI/gpt-neo-2.7B', do_lower_case=False, additional_special_tokens=['[Prompt]'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoFxbOVW91Rc"
      },
      "outputs": [],
      "source": [
        "model = model_class_n.from_pretrained(gen_model_name_or_path, load_in_half_prec=True)\n",
        "model = model.to(device)\n",
        "model = model.float()\n",
        "model.config.use_cache=True\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "gedi_model_name_or_path = 'fortune_gedi' #'sst5_lr2e-05' # 'sst5_delib_100_model'\n",
        "gedi_model = model_class_2.from_pretrained(gedi_model_name_or_path)\n",
        "gedi_model.to(device)\n",
        "gedi_model.resize_token_embeddings(len(tokenizer))\n",
        "gedi_model.resize_token_embeddings(50258)\n",
        "wte = gedi_model.get_input_embeddings()\n",
        "wte.weight.requires_grad=False\n",
        "wte.weight[len(tokenizer)-1, :]= wte.weight[len(tokenizer)-2, :]\n",
        "gedi_model.set_input_embeddings(wte)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uagkl2d6949A"
      },
      "outputs": [],
      "source": [
        "embed_cont = torch.load('./result_embedding_cont')\n",
        "embed_infill_front = torch.load('./result_embedding_infill_front')\n",
        "embed_infill_back = torch.load('./result_embedding_infill_back')\n",
        "embed_recognition = torch.load('./result_embedding_recognition')\n",
        "recognition_score = torch.load('./recog_score')\n",
        "model.set_input_embeddings(embed_cont.wte)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljyDymFr99fg"
      },
      "outputs": [],
      "source": [
        "# setting arguments for generation\n",
        "#max generation length\n",
        "gen_length = 40\n",
        "#omega from paper, higher disc_weight means more aggressive topic steering\n",
        "disc_weight = 30\n",
        "#1 - rho from paper, should be between 0 and 1 higher filter_p means more aggressive topic steering\n",
        "filter_p = 0.8\n",
        "#tau from paper, preserves tokens that are classified as correct topic\n",
        "target_p = 0.8\n",
        "#hyperparameter that determines class prior, set to uniform by default\n",
        "class_bias = 0\n",
        "\n",
        "if gen_length>1024:\n",
        "  length = 1024\n",
        "else:\n",
        "  length = gen_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSWCB2ok-BOQ"
      },
      "outputs": [],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def cut_into_sentences(text, do_cleanup=True):\n",
        "    \"\"\"\n",
        "    Cut text into sentences. \\n are also regarded as a sentence.\n",
        "    :param do_cleanup: if True, do cleanups.\n",
        "    :param text: input text.\n",
        "    :return: sentences.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "    # print(text)\n",
        "    # sentences_raw = text.split(\"\\n\")\n",
        "    text = text.replace(\"[Prompt] [Prompt] [Prompt] [Prompt] \", \"[Prompt] [Prompt] [Prompt] \")\n",
        "    sentences_raw = text.split('[Prompt] [Prompt] [Prompt]')\n",
        "    text = sentences_raw[3]\n",
        "    text = text.replace(\"Start:\", \" \")\n",
        "    text = text.replace(\"Characters:\", \" \")\n",
        "    text = text.replace(\"Story after start:\", \" \")\n",
        "    sentences_raw = [text.replace(\"\\n\", \" \")]\n",
        "    result = []\n",
        "\n",
        "    for item in sentences_raw:\n",
        "        sentence_in_item = sent_tokenize(item)\n",
        "        for item2 in sentence_in_item:\n",
        "            all_sentences.append(item2.strip())\n",
        "\n",
        "    if do_cleanup:\n",
        "        for item in all_sentences:\n",
        "            item = item.replace('<|endoftext|>', '')\n",
        "            if item[len(item)-1] not in ['.', '!', '?']:\n",
        "              item = item + '.'\n",
        "            if len(item) > 2:\n",
        "                result.append(item)\n",
        "    else:\n",
        "        result = all_sentences\n",
        "\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsHIEnEG-EcO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_one_sentence(sentence, control, length=50, disc_weight=30, temperature=0.8, gpt3_id=None):\n",
        "    \"\"\"\n",
        "    Generate one sentence based on input data.\n",
        "    :param sentence: (string) context (prompt) used.\n",
        "    :param topic: (dict) {topic: weight, topic:weight,...} topic that the sentence need to steer towards.\n",
        "    :param extra_args: (dict) a dictionary that certain key will trigger additional functionality.\n",
        "        disc_weight: Set this value to use a different control strength than default.\n",
        "        get_gen_token_count: Return only how many tokens the generator has generated (for debug only).\n",
        "    :return: sentence generated, or others if extra_args are specified.\n",
        "    \"\"\"\n",
        "    secondary_code = control\n",
        "\n",
        "    # disc_weight = self.disc_weight\n",
        "    # if type(extra_args) is dict and 'disc_weight' in extra_args:\n",
        "    #     disc_weight = extra_args['disc_weight']\n",
        "\n",
        "    if sentence == \"\":\n",
        "        print(\"Prompt is empty! Using a dummy sentence.\")\n",
        "        sentence = \".\"\n",
        "\n",
        "    # Specify prompt below\n",
        "    prompt = sentence\n",
        "\n",
        "    # Calculate oroginal input length.\n",
        "    length_of_prompt = len(sentence)\n",
        "\n",
        "    start_len = 0\n",
        "    text_ids = tokenizer.encode(prompt)\n",
        "    length_of_prompt_in_tokens = len(text_ids)\n",
        "    # print('text ids', text_ids)\n",
        "    \n",
        "    encoded_prompts = torch.LongTensor(text_ids).unsqueeze(0).to(device)\n",
        "\n",
        "    if type(control) is str:\n",
        "        multi_code = tokenizer.encode(secondary_code)\n",
        "    elif type(control) is dict:\n",
        "        multi_code = {}\n",
        "        for item in secondary_code:\n",
        "            encoded = tokenizer.encode(item)[0]  # only take the first one\n",
        "            multi_code[encoded] = secondary_code[item]\n",
        "    else:\n",
        "        raise NotImplementedError(\"topic data type of %s not supported... Supported: (str,dict)\" % type(control))\n",
        "\n",
        "    # If 1, generate sentences towards a specific topic.\n",
        "    attr_class = 1\n",
        "    print(multi_code)\n",
        "\n",
        "    if int(control)!=-1:\n",
        "      if gpt3_id is None:\n",
        "        generated_sequence = model.generate(input_ids=encoded_prompts,\n",
        "                                                  pad_lens=None,\n",
        "                                                  max_length=length + length_of_prompt_in_tokens,\n",
        "                                                  top_k=None,\n",
        "                                                  top_p=None,\n",
        "                                                  repetition_penalty=1.2,\n",
        "                                                  rep_penalty_scale=10,\n",
        "                                                  eos_token_ids=tokenizer.eos_token_id,\n",
        "                                                  pad_token_id=tokenizer.eos_token_id,\n",
        "                                                  bad_token_ids = tokenizer.all_special_ids,\n",
        "                                                  do_sample=True,\n",
        "                                                  temperature = temperature,\n",
        "                                                  penalize_cond=True,\n",
        "                                                  gedi_model=gedi_model,\n",
        "                                                  tokenizer=tokenizer,\n",
        "                                                  disc_weight=disc_weight,\n",
        "                                                  filter_p=filter_p,\n",
        "                                                  target_p=target_p,\n",
        "                                                  class_bias=class_bias,\n",
        "                                                  attr_class=attr_class,\n",
        "                                                  code_0=code_undesired,\n",
        "                                                  code_1=code_desired,\n",
        "                                                  multi_code=multi_code,\n",
        "                                                  )\n",
        "      else: \n",
        "        generated_sequence = model.generate(input_ids=encoded_prompts,\n",
        "                                                  pad_lens=None,\n",
        "                                                  max_length=length + length_of_prompt_in_tokens,\n",
        "                                                  top_k=None,\n",
        "                                                  top_p=None,\n",
        "                                                  repetition_penalty=1.2,\n",
        "                                                  rep_penalty_scale=10,\n",
        "                                                  eos_token_ids=tokenizer.eos_token_id,\n",
        "                                                  pad_token_id=tokenizer.eos_token_id,\n",
        "                                                  bad_token_ids = tokenizer.all_special_ids,\n",
        "                                                  do_sample=True,\n",
        "                                                  temperature = temperature,\n",
        "                                                  penalize_cond=True,\n",
        "                                                  gedi_model=gedi_model,\n",
        "                                                  tokenizer=tokenizer,\n",
        "                                                  disc_weight=disc_weight,\n",
        "                                                  filter_p=filter_p,\n",
        "                                                  target_p=target_p,\n",
        "                                                  class_bias=class_bias,\n",
        "                                                  attr_class=attr_class,\n",
        "                                                  code_0=code_undesired,\n",
        "                                                  code_1=code_desired,\n",
        "                                                  multi_code=multi_code,\n",
        "                                                  gpt3_api_key=gpt3_id,\n",
        "                                                  )\n",
        "      text = tokenizer.decode(generated_sequence.tolist()[0])\n",
        "    else:\n",
        "      if gpt3_id is None:\n",
        "        generated_sequence = model.generate(input_ids=encoded_prompts,\n",
        "                                                  pad_lens=None,\n",
        "                                                  max_length=length + length_of_prompt_in_tokens,\n",
        "                                                  top_k=None,\n",
        "                                                  top_p=None,\n",
        "                                                  repetition_penalty=1.2,\n",
        "                                                  rep_penalty_scale=10,\n",
        "                                                  eos_token_ids=tokenizer.eos_token_id,\n",
        "                                                  pad_token_id=tokenizer.eos_token_id,\n",
        "                                                  bad_token_ids = tokenizer.all_special_ids,\n",
        "                                                  do_sample=True,\n",
        "                                                  temperature = temperature, \n",
        "                                                  penalize_cond=True,\n",
        "                                                  gedi_model=None,\n",
        "                                                  tokenizer=tokenizer,\n",
        "                                                  disc_weight=disc_weight,\n",
        "                                                  class_bias=class_bias,\n",
        "                                                  attr_class=attr_class,\n",
        "                                                  )\n",
        "        text = tokenizer.decode(generated_sequence.tolist()[0])\n",
        "        \n",
        "\n",
        "        \n",
        "      else:\n",
        "        import openai\n",
        "        openai.api_key = gpt3_id\n",
        "        completion = openai.Completion()\n",
        "        response = completion.create(prompt=prompt,\n",
        "                                 engine=\"curie\",\n",
        "                                 max_tokens=length,\n",
        "                                 temperature=temperature,)\n",
        "        text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    \n",
        "    text = cut_into_sentences(text)\n",
        "    if len(text) == 0:\n",
        "        print(\"Warning! No text generated.\")\n",
        "        return \"\"\n",
        "    all_gen_text = text[0]\n",
        "    return all_gen_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNgTsZMW-Hqw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def continuing_generation(prompts, generation_controls, characters, temperatures, gpt3_id=None, disc_weight=30):\n",
        "  \"\"\"\n",
        "  Explanations on controls\n",
        "  prompts: The prompt to be input. This is a list of sentences. \n",
        "  generation_controls: Generation control in the list. If no control is given, -1 is given.\n",
        "  \n",
        "  \"\"\"\n",
        "  model.set_input_embeddings(embed_cont)\n",
        "  prompts = list(prompts)\n",
        "  generated = []\n",
        "\n",
        "  character_prepend = '[Prompt][Prompt][Prompt]'\n",
        "  for idx, character in enumerate(characters):\n",
        "    if idx==0:\n",
        "      character_prepend = character_prepend+character\n",
        "    else:\n",
        "      character_prepend = character_prepend+' '+character\n",
        "    if idx != len(characters)-1:\n",
        "      character_prepend = character_prepend + ','\n",
        "\n",
        "  prompt_start_idx = 0\n",
        "  for c_idx, generation_control in enumerate(generation_controls):\n",
        "    \n",
        "    temperature = temperatures[c_idx]\n",
        "    while True:\n",
        "      prompt_postpend = '[Prompt][Prompt][Prompt]'\n",
        "      # prompt_postpend = 'Story: '\n",
        "\n",
        "      for i in range(prompt_start_idx, len(prompts)):\n",
        "        prompt_postpend = prompt_postpend + prompts[i]\n",
        "        if i != len(prompts)-1:\n",
        "          prompt_postpend = prompt_postpend + ' '\n",
        "          # continue\n",
        "        else:\n",
        "          prompt_postpend = prompt_postpend\n",
        "      \n",
        "      prompt_input = prompt_postpend+character_prepend+ '[Prompt][Prompt][Prompt]'\n",
        "      prompt_encoded = tokenizer.encode(prompt_input)\n",
        "      length_of_prompt_in_tokens = len(prompt_encoded)\n",
        "      if length_of_prompt_in_tokens>2048:\n",
        "        prompt_start_idx = prompt_start_idx + 1\n",
        "      else:\n",
        "        break\n",
        "    print(prompt_input, generation_control)\n",
        "    gen_sent = generate_one_sentence(prompt_input, generation_control, temperature=temperature, gpt3_id=gpt3_id, disc_weight=disc_weight)\n",
        "    prompts.append(gen_sent)\n",
        "    generated.append(gen_sent)\n",
        "  \n",
        "  for gen in generated:\n",
        "    print('gen:', gen)\n",
        "    print()\n",
        "  return generated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aSX1y9V-bgq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def infilling_generation(pre_prompts, post_prompts, generation_controls, characters, temperatures, is_front, gpt3_id=None, disc_weight=30):\n",
        "  \"\"\"\n",
        "  Explanations on controls\n",
        "  prompts: The prompt to be input. This is a list of sentences. \n",
        "  generation_controls: Generation control in the list. If no control is given, -1 is given.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  pre_prompts = list(pre_prompts)\n",
        "  post_prompts = list(post_prompts)\n",
        "  right = ''\n",
        "  for idx, pp in enumerate(post_prompts):\n",
        "    right = right + pp\n",
        "    if idx!=len(post_prompts)-1:\n",
        "      right = right + ' '\n",
        "  left = ''\n",
        "  for idx, pp in enumerate(pre_prompts):\n",
        "    left = left + pp\n",
        "    if idx!=len(post_prompts)-1:\n",
        "      left = left + ' '\n",
        "  generated = ['']*len(generation_controls)\n",
        "\n",
        "  # gen_counter = 0\n",
        "  for gen_counter in range(len(generation_controls)):\n",
        "    if is_front:\n",
        "      generation_control = generation_controls[int(gen_counter/2)]\n",
        "      temperature = temperatures[int(gen_counter/2)]\n",
        "      model.set_input_embeddings(embed_infill_front)\n",
        "      prompt_input = '[Prompt][Prompt][Prompt]'+right+'[Prompt][Prompt][Prompt]'+left+'[Prompt][Prompt][Prompt][Prompt]'\n",
        "      \n",
        "      gen_sent = generate_one_sentence(prompt_input, generation_control, temperature=temperature, gpt3_id=gpt3_id, disc_weight=disc_weight)\n",
        "      generated[int(gen_counter/2)] =gen_sent\n",
        "      print(gen_sent)\n",
        "      left = left + ' ' + gen_sent\n",
        "    else:\n",
        "      generation_control = generation_controls[len(generated)-1-int(gen_counter/2)]\n",
        "      temperature = temperatures[len(generated)-1-int(gen_counter/2)]\n",
        "      model.set_input_embeddings(embed_infill_back)\n",
        "      prompt_input = '[Prompt][Prompt][Prompt]'+left+'[Prompt][Prompt][Prompt]'+right + '[Prompt][Prompt][Prompt][Prompt]' \n",
        "      gen_sent = generate_one_sentence(prompt_input, generation_control, temperature=temperature, gpt3_id=gpt3_id, disc_weight=disc_weight)\n",
        "      generated[len(generated)-1-int(gen_counter/2)] =gen_sent\n",
        "      print(gen_sent)\n",
        "      right = gen_sent+' '+right\n",
        "\n",
        "  for gen in generated:\n",
        "    print('gen', gen)\n",
        "    print()\n",
        "  return generated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRWlTwp1VXwJ"
      },
      "outputs": [],
      "source": [
        "def recognize_sentence_fortune(pre_context, character, target_sentence):\n",
        "  rec_input = \"[Prompt][Prompt][Prompt]\"+pre_context+\"[Prompt][Prompt][Prompt]\"+character+\"[Prompt][Prompt][Prompt]\"+target_sentence\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    model.set_input_embeddings(embed_recognition)\n",
        "    tokenized_input = tokenizer.encode(rec_input)\n",
        "    tokenized_input = torch.LongTensor(tokenized_input).unsqueeze(0).to(device)\n",
        "    output = model.transformer(tokenized_input)\n",
        "    op= output[0].type(torch.half)\n",
        "    \n",
        "    # op=output[0].type(torch.FloatTensor).to(device)\n",
        "    logits = recognition_score(op)\n",
        "    \n",
        "    to_return = float(logits[0][len(tokenized_input[0])-1][0])\n",
        "    if to_return > 1:\n",
        "      to_return = 1\n",
        "    elif to_return <0:\n",
        "      to_return = 0\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkQEO7Ef9BNu"
      },
      "source": [
        "# Server code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx6EuBPhW7wZ"
      },
      "outputs": [],
      "source": [
        "app = FlaskAPI(__name__)\n",
        "run_with_ngrok(app)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "\n",
        "# Below is temporary function with sentiment analysis.\n",
        "# Hence, it needs to be updated later.\n",
        "@app.route('/labelSentence', methods=['GET', 'POST'])\n",
        "@cross_origin(origin='*',headers=['Content-Type'])\n",
        "def sentence_analysis():\n",
        "    print(request)\n",
        "    if request.method == 'POST':\n",
        "        print('data', request.data)\n",
        "        sentence = request.data['sentence']\n",
        "        pre_context = request.data['pre_context']\n",
        "        character = request.data['character']\n",
        "        # print(images, group_model, l2t, dec)\n",
        "\n",
        "        value = recognize_sentence_fortune(pre_context, character, sentence)\n",
        "        value = value * 100\n",
        "\n",
        "        return {'value': value}\n",
        "    return {'result':'no post'}\n",
        "\n",
        "@app.route('/continuingGeneration', methods=['GET', 'POST'])\n",
        "@cross_origin(origin='*',headers=['Content-Type'])\n",
        "def continuingGeneration():\n",
        "  if request.method == 'POST':\n",
        "      data = json.loads(request.get_data())\n",
        "      pre_context = json.loads(data['pre_context'])\n",
        "      controls = json.loads(data['controls'])\n",
        "      characters = json.loads(data['characters'])\n",
        "      temperature = json.loads(data['temperature'])\n",
        "      regeneration = json.loads(data['regeneration'])\n",
        "      print(pre_context)\n",
        "      print(controls)\n",
        "      print(characters)\n",
        "      print(temperature)\n",
        "      counter = 0\n",
        "\n",
        "      value_options = []\n",
        "      gen_options = []\n",
        "      diff_options = []\n",
        "    #   return jsonify({'generated': json.dumps(['generated']), 'values': json.dumps([50])})\n",
        "      nothing_counter = 0\n",
        "      while True:\n",
        "        generated = continuing_generation(pre_context, controls, characters, temperature, gpt3_id=None, disc_weight=1)\n",
        "        gen = generated[0]\n",
        "        if len(gen)==0:\n",
        "          nothing_counter=nothing_counter+1\n",
        "          if nothing_counter<5:\n",
        "            continue\n",
        "        pre_context_concat = ''\n",
        "        start_id = len(pre_context)-1\n",
        "        if start_id<0:\n",
        "          start_id=0\n",
        "        for idx in range(start_id, len(pre_context)):\n",
        "          pre_context_concat = pre_context_concat +' '+ pre_context[idx]\n",
        "        value = recognize_sentence_fortune(pre_context_concat, characters[0], gen)\n",
        "        diff = np.abs(value*100-int(controls[0]))\n",
        "        value_options.append(value)\n",
        "        gen_options.append(gen)\n",
        "        diff_options.append(diff)\n",
        "        if diff<20 or counter==regeneration[0]:\n",
        "          minidx = np.argmin(diff_options)\n",
        "          value= value_options[minidx]\n",
        "          gen = gen_options[minidx]\n",
        "          break\n",
        "        counter = counter+1\n",
        "\n",
        "      return {'generated': json.dumps([gen]), 'values': json.dumps([value*100])}\n",
        "  return jsonify({'result': 'nothing'})\n",
        "\n",
        "@app.route('/infillingGeneration', methods=['GET', 'POST'])\n",
        "@cross_origin(origin='*',headers=['Content-Type'])\n",
        "def infillingGeneration():\n",
        "  if request.method == 'POST':\n",
        "      data = json.loads(request.get_data())\n",
        "      print(data)\n",
        "      pre_context = json.loads(data['pre_context'])\n",
        "      post_context = json.loads(data['post_context'])\n",
        "      controls = json.loads(data['controls'])\n",
        "      characters = json.loads(data['characters'])\n",
        "      temperature = json.loads(data['temperature'])\n",
        "      regeneration = json.loads(data['regeneration'])\n",
        "      print('here?')\n",
        "      is_front = data['is_front']\n",
        "      print(pre_context)\n",
        "      print(post_context)\n",
        "      print(controls)\n",
        "      print(characters)\n",
        "      print(temperature)\n",
        "      print(is_front)\n",
        "      counter = 0\n",
        "\n",
        "      value_options = []\n",
        "      gen_options = []\n",
        "      diff_options = []\n",
        "      nothing_counter = 0\n",
        "      while True:\n",
        "        generated = infilling_generation(pre_context, post_context, controls, characters, temperature, is_front, gpt3_id=None, disc_weight=1)\n",
        "        gen = generated[0]\n",
        "        if len(gen)==0:\n",
        "          nothing_counter=nothing_counter+1\n",
        "          if nothing_counter<5:\n",
        "            continue\n",
        "        pre_context_concat = ''\n",
        "        if is_front:\n",
        "          start_id = len(pre_context)-1\n",
        "          if start_id<0:\n",
        "            start_id=0\n",
        "          for idx in range(start_id, len(pre_context)):\n",
        "            pre_context_concat = pre_context_concat +' '+ pre_context[idx]\n",
        "        value = recognize_sentence_fortune(pre_context_concat, characters[0], gen)\n",
        "        diff = np.abs(value*100-int(controls[0]))\n",
        "        print(diff)\n",
        "        value_options.append(value)\n",
        "        gen_options.append(gen)\n",
        "        diff_options.append(diff)\n",
        "        if diff<20 or counter==regeneration[0]:\n",
        "          minidx = np.argmin(diff_options)\n",
        "          value= value_options[minidx]\n",
        "          gen = gen_options[minidx]\n",
        "          break\n",
        "        counter = counter+1\n",
        "      # generated = ['This is a generated sentence'] * len(controls)\n",
        "      # it needs to be updated\n",
        "      # values = sentences_analysis(generated)\n",
        "      print(gen, value)\n",
        "      return {'generated': json.dumps([gen]), 'values': json.dumps([value*100])}\n",
        "  return jsonify({'result': 'nothing'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV8WhwqPXEQr"
      },
      "outputs": [],
      "source": [
        "if __name__==\"__main__\":\n",
        "    app.run()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Mi8K8tPdoAbC"
      ],
      "machine_shape": "hm",
      "name": "TaleBrush Server Code (public).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('bitcoin-charts')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ca54cb63d7e44c50718223b064d0c7b02437e0368c3d9257ef1a652d69f5fb9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
